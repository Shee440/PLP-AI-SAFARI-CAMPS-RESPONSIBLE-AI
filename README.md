## AI Detective Blog: Investigating Two Suspicious AI Cases 🕵️‍♂️

3| Case 1: The "Hiring Bot" That’s Ghosting Women 
4| 
5| What’s the AI doing?  
6| A company uses an AI to screen job applications. It scans resumes, evaluates qualifications, and—uh oh—seems to **reject more female applicants with career gaps.  
7| 
8| What’s going wrong?  
9| 🚩 Bias Alert! The AI might be penalizing career gaps unfairly, assuming they signal "less committed" candidates—a stereotype that disproportionately affects women.  
10| 🚩 Lack of Transparency:Does the company even know why the AI rejects these applicants? Probably not.  
11| 🚩 Accountability Gap: If the AI is making biased calls, who’s responsible? The HR team? The developers?  
12| 
13| How to Fix It?
14| ✅ Audit & Retrain the AI – Test the model on historical hiring data to see if it discriminates. If so, retrain it to ignore irrelevant factors and focus on skills.  
15| ✅ Human Oversight– No AI should make final hiring decisions alone. HR should review flagged rejections to catch bias.  
16| 
17|   
18| 
19| Case 2: The "Big Brother" Proctoring AI That’s Flagging Neurodivergent Students
20| 
21| What’s the AI doing?  
22| A school uses an AI to monitor students during exams via webcam. It flags cheating based on eye movements but keeps accusing neurodivergent students (e.g., those with ADHD or autism) of misconduct[...]  
23| 
24| What’s going wrong? 
25| 🚩 False Positives Galore! Neurodivergent students may have different eye contact patterns but that doesn’t mean they’re cheating.  
26| 🚩 Privacy Concerns – Constant surveillance feels invasive, especially for students already struggling with anxiety.  
27| 🚩 No Human Judgment – The AI makes snap decisions without context, leading to unfair punishments.  
28| 
29| How to Fix It?
30| ✅ Customize Detection Rules– Train the AI to recognize **neurodiverse behaviors** and adjust sensitivity.  
31| ✅ Human Review First – Instead of auto-flagging, the AI should alert a teacher to **investigate before taking action.

## AI Detective Blog: Investigating Two Suspicious AI Cases ğŸ•µï¸â€â™‚ï¸

**Case 1: The "Hiring Bot" Thatâ€™s Ghosting Women**

 Whatâ€™s the AI doing?  
 A company uses an AI to screen job applications. It scans resumes, evaluates qualifications, andâ€”uh ohâ€”seems to **reject more female applicants with career gaps.  
 
 Whatâ€™s going wrong? 
 ğŸš© Bias Alert! The AI might be penalizing career gaps unfairly, assuming they signal "less committed" candidatesâ€”a stereotype that disproportionately affects women.  
 ğŸš© Lack of Transparency:Does the company even know why the AI rejects these applicants? Probably not.  
 ğŸš© Accountability Gap: If the AI is making biased calls, whoâ€™s responsible? The HR team? The developers?  
 
 How to Fix It?
 âœ… Audit & Retrain the AI â€“ Test the model on historical hiring data to see if it discriminates. If so, retrain it to ignore irrelevant factors and focus on skills.  
 âœ… Human Oversightâ€“ No AI should make final hiring decisions alone. HR should review flagged rejections to catch bias.  
    
## Case 2: The "Big Brother" Proctoring AI Thatâ€™s Flagging Neurodivergent Students

Whatâ€™s the AI doing?  
A school uses an AI to monitor students during exams via webcam. It flags cheating based on eye movements but keeps accusing neurodivergent students (e.g., those with ADHD or autism) of misconduct.   

Whatâ€™s going wrong? 
ğŸš© False Positives Galore! Neurodivergent students may have different eye contact patterns but that doesnâ€™t mean theyâ€™re cheating.  
ğŸš© Privacy Concerns â€“ Constant surveillance feels invasive, especially for students already struggling with anxiety.  
ğŸš© No Human Judgment â€“ The AI makes snap decisions without context, leading to unfair punishments.  
 
How to Fix It?
âœ… Customize Detection Rulesâ€“ Train the AI to recognize **neurodiverse behaviors** and adjust sensitivity.  
âœ… Human Review First â€“ Instead of auto-flagging, the AI should alert a teacher to **investigate before taking action.
